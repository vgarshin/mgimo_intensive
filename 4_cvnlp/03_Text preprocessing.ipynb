{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5191ba01-86ca-4697-917f-4b53566ab90c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MGIMO intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c13a0-9590-4303-aea8-87ac464665d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text data base processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57fa59-c316-4b56-bd81-365d6d2f3e33",
   "metadata": {},
   "source": [
    "Notebook is generated with help of DeepSeek with prompt:\n",
    "```prompt\n",
    "## Базовые параметры\n",
    "- Роль: Junior Python Developer (Data Analyst)\n",
    "- Специализация: data analysis, data visualization, NLP\n",
    "- Уровень: начинающий\n",
    "- Температура: 0 (максимальная точность и предсказуемость)\n",
    "\n",
    "## Контекст выполнения\n",
    "Разработка кода для предобработки и первичного анализа сырого текста по биоразнообразию\n",
    "\n",
    "## Входные данные\n",
    "- файл `text.txt` на диске\n",
    "- в файле содержится отрывок из описания заповедника на русском языке\n",
    "\n",
    "## Технические ограничения\n",
    "- Использовать библиотеки `re`, `nltk`, `spacy` и стандартные библиотеки Python\n",
    "- Код запускается в интерактивном ноутбуке Jupyter\n",
    "- Визуализация и диаграммы для интерактивного ноутбука\n",
    "\n",
    "## Требования к реализации\n",
    "\n",
    "### Функциональные требования\n",
    "1. Требуется провести анализ различными библиотеками и сравнить результаты\n",
    "2. Требуется извлечение именованных сущностей и таблиц \n",
    "3. Визуализация wordcloud для описания текста \n",
    "4. Необходимо продемонстрировать несколько вариантов токенизации текста\n",
    "\n",
    "### Технические требования\n",
    "- Сложность кода: не используй классы, ограничься функциями\n",
    "- Архитектура: упрощенная, для использования в интерактивных ноутбуках\n",
    "- Стиль кода: PEP 8, black (длина строки 79)\n",
    "- Документация: Docstrings в стиле Google\n",
    "- Безопасность: Никаких захардкоженных credentials\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3732d-3b06-4716-952e-6eda173f5c9b",
   "metadata": {},
   "source": [
    "### 1. Libraries install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d397bbd-7456-44ff-b623-34e09962b2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397754cf-18af-4507-ab6d-32431318aef1",
   "metadata": {},
   "source": [
    "### 2. Libraries and downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8174b8-5480-4328-8d70-f95c5f07fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Загрузка необходимых ресурсов NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Загрузка русской модели spaCy\n",
    "try:\n",
    "    nlp = spacy.load('ru_core_news_sm')\n",
    "except OSError:\n",
    "    print(\"Модель 'ru_core_news_sm' не найдена.\")\n",
    "    print(\"Установите её командой: python -m spacy download ru_core_news_sm\")\n",
    "    # Можно использовать пустую модель для демонстрации, но NER не будет работать\n",
    "    # nlp = spacy.blank('ru')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239bb51-fe08-4bb2-85d9-b85b6f758a62",
   "metadata": {},
   "source": [
    "### 3. Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c774124-b3a4-4c87-b7f2-3293793e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filepath: str) -> str:\n",
    "    \"\"\"Загружает текст из файла.\n",
    "\n",
    "    Args:\n",
    "        filepath: Путь к текстовому файлу.\n",
    "\n",
    "    Returns:\n",
    "        Содержимое файла в виде строки.\n",
    "    \"\"\"\n",
    "    path = Path(filepath)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Файл {filepath} не найден.\")\n",
    "    return path.read_text(encoding='utf-8')\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Базовая очистка текста от лишних пробелов и спецсимволов.\n",
    "\n",
    "    Args:\n",
    "        text: Исходный текст.\n",
    "\n",
    "    Returns:\n",
    "        Очищенный текст.\n",
    "    \"\"\"\n",
    "    # Замена множественных пробелов и переносов строк на один пробел\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Удаление символов, не являющихся буквами, цифрами или знаками препинания\n",
    "    # Оставляем точки и запятые, так как они важны для токенизации предложений\n",
    "    text = re.sub(r'[^а-яА-ЯёЁ0-9\\s\\.\\,\\-\\–\\(\\)]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenize_with_nltk(text: str) -> tuple:\n",
    "    \"\"\"Токенизация текста с помощью NLTK.\n",
    "\n",
    "    Args:\n",
    "        text: Исходный текст.\n",
    "\n",
    "    Returns:\n",
    "        Кортеж (список предложений, список слов).\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text, language='russian')\n",
    "    words = word_tokenize(text.lower(), language='russian')\n",
    "    # Оставляем только слова (без пунктуации)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    return sentences, words\n",
    "\n",
    "\n",
    "def tokenize_with_spacy(text: str) -> tuple:\n",
    "    \"\"\"Токенизация текста с помощью spaCy.\n",
    "\n",
    "    Args:\n",
    "        text: Исходный текст.\n",
    "\n",
    "    Returns:\n",
    "        Кортеж (список предложений, список слов).\n",
    "    \"\"\"\n",
    "    if 'nlp' not in globals() or nlp is None:\n",
    "        raise RuntimeError(\"Модель spaCy не загружена.\")\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    # Приводим к нижнему регистру и оставляем только слова\n",
    "    words = [token.text.lower() for token in doc if token.is_alpha]\n",
    "    return sentences, words\n",
    "\n",
    "\n",
    "def compare_tokenization(text: str):\n",
    "    \"\"\"Сравнивает результаты токенизации NLTK и spaCy.\n",
    "\n",
    "    Args:\n",
    "        text: Исходный текст.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"СРАВНЕНИЕ ТОКЕНИЗАЦИИ\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # NLTK токенизация\n",
    "    nltk_sentences, nltk_words = tokenize_with_nltk(text)\n",
    "    print(f\"\\nNLTK:\")\n",
    "    print(f\"  Предложений: {len(nltk_sentences)}\")\n",
    "    print(f\"  Слов: {len(nltk_words)}\")\n",
    "    if nltk_sentences:\n",
    "        print(f\"  Первые 3 предложения: {nltk_sentences[:3]}\")\n",
    "\n",
    "    # spaCy токенизация\n",
    "    try:\n",
    "        spacy_sentences, spacy_words = tokenize_with_spacy(text)\n",
    "        print(f\"\\nspaCy:\")\n",
    "        print(f\"  Предложений: {len(spacy_sentences)}\")\n",
    "        print(f\"  Слов: {len(spacy_words)}\")\n",
    "        if spacy_sentences:\n",
    "            print(f\"  Первые 3 предложения: {spacy_sentences[:3]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\nspaCy: {e}\")\n",
    "\n",
    "\n",
    "def extract_entities_spacy(text: str) -> list:\n",
    "    \"\"\"Извлекает именованные сущности с помощью spaCy.\n",
    "\n",
    "    Args:\n",
    "        text: Исходный текст.\n",
    "\n",
    "    Returns:\n",
    "        Список кортежей (текст_сущности, тип_сущности).\n",
    "    \"\"\"\n",
    "    if 'nlp' not in globals() or nlp is None:\n",
    "        raise RuntimeError(\"Модель spaCy не загружена. NER недоступен.\")\n",
    "\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "\n",
    "def extract_entities_regex(text: str) -> dict:\n",
    "    \"\"\"Извлекает потенциальные названия видов с помощью регулярных выражений.\n",
    "\n",
    "    Args:\n",
    "        text: Исходный текст.\n",
    "\n",
    "    Returns:\n",
    "        Словарь с типами сущностей и найденными значениями.\n",
    "    \"\"\"\n",
    "    # Простой поиск слов с заглавной буквы (потенциальные названия видов)\n",
    "    # Ищем слова длиной больше 2 символов, начинающиеся с заглавной буквы\n",
    "    potential_species = re.findall(r'\\b[А-ЯЁ][а-яё]{2,}(?:\\s+[А-ЯЁ][а-яё]{2,})*\\b', text)\n",
    "\n",
    "    # Поиск числовых значений (площади, количества)\n",
    "    numbers = re.findall(r'\\b\\d+(?:[.,]\\d+)?\\s*(?:км|га|м|тыс|млн)\\b', text)\n",
    "\n",
    "    return {\n",
    "        'potential_species': potential_species,\n",
    "        'measurements': numbers\n",
    "    }\n",
    "\n",
    "\n",
    "def create_wordcloud(words: list, title: str = \"WordCloud\") -> None:\n",
    "    \"\"\"Создает и отображает облако слов.\n",
    "\n",
    "    Args:\n",
    "        words: Список слов для облака.\n",
    "        title: Заголовок графика.\n",
    "    \"\"\"\n",
    "    if not words:\n",
    "        print(\"Недостаточно слов для создания wordcloud.\")\n",
    "        return\n",
    "\n",
    "    # Преобразуем список слов в строку\n",
    "    text_for_cloud = ' '.join(words)\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        contour_width=1,\n",
    "        contour_color='steelblue'\n",
    "    ).generate(text_for_cloud)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_stopwords() -> set:\n",
    "    \"\"\"Получает список стоп-слов для русского языка.\n",
    "\n",
    "    Returns:\n",
    "        Множество стоп-слов.\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    # Добавляем дополнительные часто встречающиеся слова\n",
    "    extra_stops = {'это', 'весь', 'также', 'например', 'который', 'свой',\n",
    "                   'очень', 'ещё', 'уже', 'чтобы', 'может', 'быть'}\n",
    "    return russian_stopwords.union(extra_stops)\n",
    "\n",
    "\n",
    "def plot_frequency_distribution(words: list, top_n: int = 20) -> None:\n",
    "    \"\"\"Строит график частотности слов.\n",
    "\n",
    "    Args:\n",
    "        words: Список слов.\n",
    "        top_n: Количество наиболее частотных слов для отображения.\n",
    "    \"\"\"\n",
    "    stop_words = get_stopwords()\n",
    "    # Фильтруем стоп-слова и короткие слова\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "\n",
    "    freq_dist = FreqDist(filtered_words)\n",
    "    common_words = freq_dist.most_common(top_n)\n",
    "\n",
    "    if not common_words:\n",
    "        print(\"Нет данных для построения графика частотности.\")\n",
    "        return\n",
    "\n",
    "    words_list, counts = zip(*common_words)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(words_list)), counts, tick_label=words_list)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Слова')\n",
    "    plt.ylabel('Частота')\n",
    "    plt.title(f'Топ-{top_n} наиболее частотных слов')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3632d3-97e8-4238-a5f6-8274d391da08",
   "metadata": {},
   "source": [
    "### 4. Text processing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7c0dc-e630-4633-87ca-77416154e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка текста\n",
    "text = load_text('text.txt')\n",
    "\n",
    "print(\"ИСХОДНЫЙ ТЕКСТ (первые 500 символов):\")\n",
    "print(\"-\" * 50)\n",
    "print(text[:500])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Очистка текста\n",
    "cleaned_text = clean_text(text)\n",
    "print(f\"\\nДлина исходного текста: {len(text)} символов\")\n",
    "print(f\"Длина после очистки: {len(cleaned_text)} символов\")\n",
    "\n",
    "# 1. Сравнение токенизации\n",
    "compare_tokenization(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17be88-e947-4bd1-b098-3d111ea62ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Извлечение именованных сущностей\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ИЗВЛЕЧЕНИЕ СУЩНОСТЕЙ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Извлечение с помощью регулярных выражений\n",
    "regex_entities = extract_entities_regex(cleaned_text)\n",
    "print(\"\\nРегулярные выражения:\")\n",
    "for entity_type, values in regex_entities.items():\n",
    "    if values:\n",
    "        print(f\"  {entity_type}: {values[:10]}\")  # Показываем первые 10\n",
    "\n",
    "# Извлечение с помощью spaCy NER\n",
    "try:\n",
    "    spacy_entities = extract_entities_spacy(cleaned_text)\n",
    "    print(\"\\nspaCy NER:\")\n",
    "    if spacy_entities:\n",
    "        # Группируем по типам для наглядности\n",
    "        entity_types = {}\n",
    "        for entity, label in spacy_entities:\n",
    "            if label not in entity_types:\n",
    "                entity_types[label] = []\n",
    "            entity_types[label].append(entity)\n",
    "\n",
    "        for label, entities in entity_types.items():\n",
    "            print(f\"  {label}: {entities[:5]}\")  # Показываем первые 5 каждого типа\n",
    "    else:\n",
    "        print(\"  Сущности не найдены.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nspaCy NER: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa5f28-c140-4b63-b503-ec92d5f69c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Токенизация для визуализации\n",
    "# Используем NLTK для визуализации, так как он всегда доступен\n",
    "sentences, words = tokenize_with_nltk(cleaned_text)\n",
    "\n",
    "# 4. WordCloud\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ВИЗУАЛИЗАЦИЯ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Создаем облако слов (без стоп-слов)\n",
    "stop_words = get_stopwords()\n",
    "filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "create_wordcloud(filtered_words, title=\"Облако ключевых слов текста\")\n",
    "\n",
    "# График частотности\n",
    "plot_frequency_distribution(words)\n",
    "\n",
    "# Дополнительная информация\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"СТАТИСТИКА\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Всего слов (после токенизации): {len(words)}\")\n",
    "print(f\"Уникальных слов: {len(set(words))}\")\n",
    "print(f\"Всего предложений: {len(sentences)}\")\n",
    "print(f\"Средняя длина предложения: {len(words) / len(sentences):.1f} слов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d733330-9f56-43a8-b931-dd439477b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751fc4a-2970-47d1-a1b4-fb039f309500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e4a9e-ee30-4ea8-b51c-942541a5501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bba4da-7eb7-470d-8560-55f84d7b7bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
