{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MGIMO intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NTL dataset: CNN approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for use of OpenCV with Python API [see here](https://docs.opencv.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "import re\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image, ImageFile\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Включение поддержки загрузки больших изображений\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = '/home/jovyan/__DATA/mgimo_intensive/nlt/dataset'\n",
    "TARGET_SIZE = (128, 128)\n",
    "LIMIT_IMGS = 2000\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Take the dataset from Kaggle - [Country-Wise Nightlight Images Dataset](https://www.kaggle.com/datasets/abhijeetdtu/country-nightlight-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jovyan/__DATA/mgimo_intensive/nlt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -la $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(IMG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp = pd.read_csv(f\"{DATA_PATH}/gdp_melted.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_dataframe(img_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Создает датафрейм с информацией об изображениях.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Путь к директории с изображениями\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame с колонками:\n",
    "        - country_code: код страны из названия файла\n",
    "        - year: год из названия файла\n",
    "        - file_path: полный путь к файлу изображения\n",
    "        - filename: исходное имя файла\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: Если указанная директория не существует\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Директория {img_path} не найдена\")\n",
    "    \n",
    "    # Собираем информацию о файлах\n",
    "    image_data = []\n",
    "    \n",
    "    # Паттерн для извлечения кода страны и года из имени файла\n",
    "    # Пример: CHN_1997.png_0_4784.jpeg\n",
    "    pattern = re.compile(r'([A-Z]{3})_(\\d{4})')\n",
    "    \n",
    "    for filename in os.listdir(img_path):\n",
    "        if not filename.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
    "            continue\n",
    "            \n",
    "        match = pattern.search(filename)\n",
    "        if match:\n",
    "            country_code = match.group(1)\n",
    "            year = int(match.group(2))\n",
    "            \n",
    "            image_data.append({\n",
    "                'country_code': country_code,\n",
    "                'year': year,\n",
    "                'file_path': str(Path(img_path) / filename),\n",
    "                'filename': filename\n",
    "            })\n",
    "    \n",
    "    df_img = pd.DataFrame(image_data)\n",
    "    \n",
    "    if df_img.empty:\n",
    "        print(\"Предупреждение: не найдено изображений, соответствующих паттерну\")\n",
    "    \n",
    "    return df_img\n",
    "\n",
    "\n",
    "def load_and_resize_image(file_path: str, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:\n",
    "    \"\"\"Загружает изображение и изменяет его до точного размера target_size.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Путь к файлу изображения\n",
    "        target_size: Целевой размер (ширина, высота), по умолчанию (224, 224)\n",
    "        \n",
    "    Returns:\n",
    "        Изображение в виде numpy массива (224, 224) с типом float16\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Загружаем изображение сразу в grayscale\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_REDUCED_GRAYSCALE_2)\n",
    "        \n",
    "        if img is None:\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Принудительный ресайз до точного размера 224x224\n",
    "        # INTER_LINEAR хорош для увеличения, INTER_AREA для уменьшения\n",
    "        # Определяем, нужно увеличивать или уменьшать\n",
    "        height, width = img.shape[:2]\n",
    "        if height > target_size[1] or width > target_size[0]:\n",
    "            # Уменьшаем\n",
    "            interpolation = cv2.INTER_AREA\n",
    "        else:\n",
    "            # Увеличиваем\n",
    "            interpolation = cv2.INTER_LINEAR\n",
    "        \n",
    "        # Ресайз до точного размера\n",
    "        img_resized = cv2.resize(img, target_size, interpolation=interpolation)\n",
    "        \n",
    "        # Конвертируем в float16 и нормализуем\n",
    "        img_array = img_resized / 255.0\n",
    "        \n",
    "        # Очищаем память\n",
    "        del img, img_resized\n",
    "        gc.collect()\n",
    "        \n",
    "        return img_array\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_image_features(\n",
    "    image_array: np.ndarray,\n",
    "    method: str = 'flatten'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Извлекает признаки из черно-белого изображения.\n",
    "    \n",
    "    Args:\n",
    "        image_array: Изображение в виде numpy массива (height, width)\n",
    "        method: Метод извлечения признаков\n",
    "        \n",
    "    Returns:\n",
    "        Вектор признаков\n",
    "    \"\"\"\n",
    "    if image_array is None:\n",
    "        return None\n",
    "    \n",
    "    if method == 'flatten':\n",
    "        # Для grayscale просто выпрямляем 2D массив\n",
    "        features = image_array.flatten()\n",
    "    elif method == 'mean_pooling':\n",
    "        # Усреднение по блокам для уменьшения размерности\n",
    "        h, w = image_array.shape\n",
    "        block_size = 8\n",
    "        h_blocks = h // block_size\n",
    "        w_blocks = w // block_size\n",
    "        features = image_array[:h_blocks*block_size, :w_blocks*block_size] \\\n",
    "                          .reshape(h_blocks, block_size, w_blocks, block_size) \\\n",
    "                          .mean(axis=(1, 3)) \\\n",
    "                          .flatten()\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный метод: {method}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def prepare_features_for_country_year(\n",
    "    df_img: pd.DataFrame,\n",
    "    method: str = 'flatten',\n",
    "    limit: int = 200,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Подготавливает признаки для всех изображений.\n",
    "    \n",
    "    Args:\n",
    "        df_img: Датафрейм с информацией об изображениях\n",
    "        method: Метод извлечения признаков\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame с признаками и мета-информацией\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_list = []\n",
    "\n",
    "    if limit == None: limit = len(df_img)\n",
    "    \n",
    "    for idx, row in df_img[:limit].iterrows():\n",
    "        if idx % 100 == 0:  # Прогресс каждые 100 изображений\n",
    "            print(f\"Обработано {idx} из {len(df_img)} изображений\")\n",
    "        \n",
    "        # Загружаем и уменьшаем изображение\n",
    "        img_array = load_and_resize_image(row['file_path'], TARGET_SIZE)\n",
    "        \n",
    "        if img_array is not None:\n",
    "            # Извлекаем признаки\n",
    "            features = extract_image_features(img_array, method)\n",
    "            \n",
    "            # Сохраняем признаки вместе с мета-информацией\n",
    "            feature_dict = {\n",
    "                'country_code': row['country_code'],\n",
    "                'year': row['year'],\n",
    "                'file_path': row['file_path']\n",
    "            }\n",
    "            \n",
    "            # Добавляем признаки как отдельные колонки\n",
    "            for i, feature_value in enumerate(features):\n",
    "                feature_dict[f'feature_{i}'] = feature_value\n",
    "            \n",
    "            feature_list.append(feature_dict)\n",
    "    \n",
    "    df_features = pd.DataFrame(feature_list)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def merge_with_gdp_data(\n",
    "    df_features: pd.DataFrame,\n",
    "    df_gdp: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Объединяет признаки с данными о ВВП.\n",
    "    \n",
    "    Args:\n",
    "        df_features: DataFrame с признаками изображений\n",
    "        df_gdp: DataFrame с данными о ВВП\n",
    "        \n",
    "    Returns:\n",
    "        Объединенный DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Подготавливаем данные ВВП\n",
    "    df_gdp_filtered = df_gdp[['code', 'year', 'gdp']].copy()\n",
    "    df_gdp_filtered = df_gdp_filtered.dropna(subset=['gdp'])\n",
    "    \n",
    "    # Переименовываем для объединения\n",
    "    df_gdp_filtered = df_gdp_filtered.rename(columns={'code': 'country_code'})\n",
    "    \n",
    "    # Объединяем датафреймы\n",
    "    df_merged = pd.merge(\n",
    "        df_features,\n",
    "        df_gdp_filtered,\n",
    "        on=['country_code', 'year'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"Объединено записей: {len(df_merged)}\")\n",
    "    print(f\"Уникальных стран: {df_merged['country_code'].nunique()}\")\n",
    "    print(f\"Диапазон лет: {df_merged['year'].min()} - {df_merged['year'].max()}\")\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def split_data_by_country_and_year(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Разделяет данные на train/val/test с учетом стран и временных периодов.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame с данными\n",
    "        test_size: Доля тестовой выборки\n",
    "        val_size: Доля валидационной выборки\n",
    "        \n",
    "    Returns:\n",
    "        Кортеж (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем группы по странам\n",
    "    groups = df['country_code'].values\n",
    "    \n",
    "    # Первое разделение: выделяем тестовую выборку\n",
    "    gss1 = GroupShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=test_size,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    train_val_idx, test_idx = next(\n",
    "        gss1.split(df, groups=groups)\n",
    "    )\n",
    "    \n",
    "    train_val_df = df.iloc[train_val_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    \n",
    "    # Второе разделение: выделяем валидационную из train\n",
    "    val_relative_size = val_size / (1 - test_size)\n",
    "    gss2 = GroupShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=val_relative_size,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    train_idx, val_idx = next(\n",
    "        gss2.split(\n",
    "            train_val_df,\n",
    "            groups=train_val_df['country_code'].values\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_df = train_val_df.iloc[train_idx]\n",
    "    val_df = train_val_df.iloc[val_idx]\n",
    "    \n",
    "    print(f\"Train: {len(train_df)} samples, {train_df['country_code'].nunique()} countries\")\n",
    "    print(f\"Val: {len(val_df)} samples, {val_df['country_code'].nunique()} countries\")\n",
    "    print(f\"Test: {len(test_df)} samples, {test_df['country_code'].nunique()} countries\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def prepare_X_y(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    target_col: str = 'gdp'\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Подготавливает матрицу признаков и целевую переменную.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame с данными\n",
    "        feature_cols: Список колонок с признаками\n",
    "        target_col: Название колонки с целевой переменной\n",
    "        \n",
    "    Returns:\n",
    "        Кортеж (X, y)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ШАГ 1: Создание датафрейма изображений\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_img = create_image_dataframe(IMG_PATH)\n",
    "print(f\"Найдено изображений: {len(df_img)}\")\n",
    "print(f\"Уникальных стран: {df_img['country_code'].nunique()}\")\n",
    "print(f\"Диапазон лет: {df_img['year'].min()} - {df_img['year'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 2: Извлечение признаков из изображений\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_features = prepare_features_for_country_year(df_img, method='flatten', limit=LIMIT_IMGS)\n",
    "print(f\"Размерность признаков: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 3: Объединение с данными ВВП\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_merged = merge_with_gdp_data(df_features, df_gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 4: Разделение на выборки\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_df, val_df, test_df = split_data_by_country_and_year(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN convolution as is__\n",
    "\n",
    "![Image array](imgs/cnnkernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN convolution kernels examples__\n",
    "\n",
    "![Image array](imgs/cnnkernelexamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN layers: why it works__\n",
    "\n",
    "![Image array](imgs/cnncapture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Define CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now correct our previous approach to utilize power of CNN:\n",
    "\n",
    "```prompt\n",
    "Теперь необходимо переделать модель предсказания GDP с использованием сверточных нейронных сетей\n",
    "- модель будет построена на сверточных фильтрах\n",
    "- не надо менять обработку и загрузку изображений, старайся использовать уже сделанный код\n",
    "- используй образцы кода ниже, но измени задачу с классификации на регрессию\n",
    "\n",
    "### Архитектура сети:\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding='valid')\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='valid')\n",
    "        self.dropout1 = nn.Dropout(.5)\n",
    "        self.dropout2 = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)  # you may need to comment this line for HA\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)  # you may need to comment this line for HA\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "model = SimpleCNN(NUM_CLASSES)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "### Обучение сети:\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.autograd.Variable(images)\n",
    "        labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        # Nullify gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output = model(images)\n",
    "        # compute loss based on obtained value and actual label\n",
    "        compute_loss = loss(output, labels)\n",
    "        # backward propagation\n",
    "        compute_loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # total correct predictions\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        correct += (predicted == labels).sum()\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                'Epoch {} - training [{}/{} ({:.0f}%)] loss: {:.3f}, accuracy: {:.2f}%'.format(\n",
    "                    epoch,\n",
    "                    i * len(images),\n",
    "                    len(train_loader.dataset),\n",
    "                    100 * i / len(train_loader),\n",
    "                    compute_loss.item(),\n",
    "                    float(correct * 100) / float(BATCH_SIZE * (i + 1))\n",
    "                ),\n",
    "                end='\\r'\n",
    "            )\n",
    "\n",
    "    # check total accuracy of predicted value and actual label\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = torch.autograd.Variable(images)\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        compute_loss = loss(output, labels)\n",
    "        # total labels\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # total correct predictions\n",
    "        accurate += (predicted == labels).sum()\n",
    "        accuracy_score = 100 * accurate/total\n",
    "\n",
    "    print('Epoch {} - validation loss: {:.3f}, validation accuracy: {:.2f}%        '.format(\n",
    "        epoch,\n",
    "        compute_loss.item(),\n",
    "        accuracy_score\n",
    "    ))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDPImageDataset(Dataset):\n",
    "    \"\"\"Датасет для изображений и целевых значений GDP.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], target_col: str = 'gdp'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame с данными\n",
    "            feature_cols: Список колонок с признаками (не используется, оставлено для совместимости)\n",
    "            target_col: Название колонки с целевой переменной\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        # Нормализуем GDP для стабильного обучения\n",
    "        self.gdp_scaler = StandardScaler()\n",
    "        gdp_values = self.df[target_col].values.reshape(-1, 1)\n",
    "        self.gdp_normalized = self.gdp_scaler.fit_transform(gdp_values).flatten()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Загружаем изображение (предполагаем, что оно уже обработано и сохранено)\n",
    "        # В реальности здесь должна быть загрузка из файла\n",
    "        # Но для экономии памяти предполагаем, что признаки уже извлечены\n",
    "        # и хранятся в feature_cols как плоский вектор\n",
    "        \n",
    "        # Для CNN нам нужно восстановить 2D структуру изображения\n",
    "        # Находим все колонки с признаками\n",
    "        feature_cols = [col for col in self.df.columns if col.startswith('feature_')]\n",
    "        features = row[feature_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Восстанавливаем изображение из плоского вектора\n",
    "        # Предполагаем, что изображение было 224x224 grayscale\n",
    "        img_size = int(np.sqrt(len(features)))\n",
    "        if img_size == TARGET_SIZE[0]:\n",
    "            image = features.reshape(1, TARGET_SIZE[0], TARGET_SIZE[1])\n",
    "        else:\n",
    "            # Если размер не совпадает, создаем заглушку\n",
    "            print(f\"Warning: Unexpected feature size: {len(features)}\")\n",
    "            image = np.zeros((1, TARGET_SIZE[0], TARGET_SIZE[1]), dtype=np.float32)\n",
    "        \n",
    "        # Получаем нормализованное значение GDP\n",
    "        gdp = self.gdp_normalized[idx].astype(np.float32)\n",
    "        \n",
    "        # Конвертируем в тензоры\n",
    "        image_tensor = torch.from_numpy(image)\n",
    "        gdp_tensor = torch.from_numpy(np.array([gdp]))\n",
    "        \n",
    "        return image_tensor, gdp_tensor\n",
    "\n",
    "\n",
    "class SimpleCNNRegressor(nn.Module):\n",
    "    \"\"\"Простая CNN для регрессии GDP.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Сверточные слои\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Пуллинг и дропаут\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        if TARGET_SIZE[0] == 224:\n",
    "            # Полносвязные слои\n",
    "            # После 3 пуллингов: 224 -> 112 -> 56 -> 28\n",
    "            self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "            self.fc2 = nn.Linear(512, 128)\n",
    "            self.fc3 = nn.Linear(128, 1)  # Один выход для регрессии\n",
    "        elif TARGET_SIZE[0] == 128:\n",
    "            # Полносвязные слои\n",
    "            # После 3 пуллингов: 128 -> 64 -> 32 -> 16\n",
    "            # Размер после сверток и пуллинга: 16x16\n",
    "            # Количество каналов: 128\n",
    "            # Размер для первого линейного слоя: 128 * 16 * 16 = 32768\n",
    "            self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "            self.fc2 = nn.Linear(512, 128)\n",
    "            self.fc3 = nn.Linear(128, 1)  # Один выход для регрессии\n",
    "        else:\n",
    "            raise AttributeError(\"Target size should be 128 or 224\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Блок 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Блок 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Блок 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Дропаут\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Полносвязные слои\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Линейная активация для регрессии\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def create_data_loaders(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    batch_size: int = BATCH_SIZE\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, StandardScaler]:\n",
    "    \"\"\"Создает DataLoader'ы для обучения CNN.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Обучающая выборка\n",
    "        val_df: Валидационная выборка\n",
    "        test_df: Тестовая выборка\n",
    "        feature_cols: Список колонок с признаками\n",
    "        batch_size: Размер батча\n",
    "        \n",
    "    Returns:\n",
    "        Кортеж (train_loader, val_loader, test_loader, gdp_scaler)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем датасеты\n",
    "    train_dataset = GDPImageDataset(train_df, feature_cols)\n",
    "    val_dataset = GDPImageDataset(val_df, feature_cols)\n",
    "    test_dataset = GDPImageDataset(test_df, feature_cols)\n",
    "    \n",
    "    # Создаем DataLoader'ы\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset.gdp_scaler\n",
    "\n",
    "\n",
    "def train_cnn_model(\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    gdp_scaler: StandardScaler,\n",
    "    num_epochs: int = NUM_EPOCHS,\n",
    "    learning_rate: float = LEARNING_RATE,\n",
    "    device: str = None\n",
    ") -> Tuple[nn.Module, Dict[str, List[float]]]:\n",
    "    \"\"\"Обучает CNN модель для регрессии GDP.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader для обучающей выборки\n",
    "        val_loader: DataLoader для валидационной выборки\n",
    "        test_loader: DataLoader для тестовой выборки\n",
    "        gdp_scaler: Scaler для обратной нормализации GDP\n",
    "        num_epochs: Количество эпох\n",
    "        learning_rate: Скорость обучения\n",
    "        device: Устройство для обучения (cuda/cpu)\n",
    "        \n",
    "    Returns:\n",
    "        Кортеж (обученная модель, история обучения)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Определяем устройство\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Используется устройство: {device}\")\n",
    "    \n",
    "    # Создаем модель\n",
    "    model = SimpleCNNRegressor().to(device)\n",
    "    \n",
    "    # Оптимизатор и функция потерь (MSE для регрессии)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # История обучения\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_mape': []\n",
    "    }\n",
    "    \n",
    "    # Обучение\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for i, (images, gdp_norm) in enumerate(train_loader):\n",
    "            # Перемещаем на устройство\n",
    "            images = images.to(device)\n",
    "            gdp_norm = gdp_norm.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, gdp_norm)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Вывод прогресса\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch} - training [{i * len(images)}/{len(train_loader.dataset)} '\n",
    "                    f'({100. * i / len(train_loader):.0f}%)] loss: {loss.item():.4f}',\n",
    "                    end='\\r'\n",
    "                )\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        val_predictions = []\n",
    "        val_actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, gdp_norm in val_loader:\n",
    "                images = images.to(device)\n",
    "                gdp_norm = gdp_norm.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, gdp_norm)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Сохраняем для вычисления MAPE\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_actuals.extend(gdp_norm.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Вычисляем MAPE (обратная нормализация GDP)\n",
    "        val_predictions = np.array(val_predictions).reshape(-1, 1)\n",
    "        val_actuals = np.array(val_actuals).reshape(-1, 1)\n",
    "        \n",
    "        val_predictions_orig = gdp_scaler.inverse_transform(val_predictions)\n",
    "        val_actuals_orig = gdp_scaler.inverse_transform(val_actuals)\n",
    "        \n",
    "        # Избегаем деления на ноль\n",
    "        mask = val_actuals_orig.flatten() != 0\n",
    "        if mask.any():\n",
    "            mape = np.mean(np.abs(\n",
    "                (val_actuals_orig.flatten()[mask] - val_predictions_orig.flatten()[mask]) / \n",
    "                val_actuals_orig.flatten()[mask]\n",
    "            )) * 100\n",
    "        else:\n",
    "            mape = np.inf\n",
    "        \n",
    "        history['val_mape'].append(mape)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch} - train loss: {avg_train_loss:.4f}, '\n",
    "              f'val loss: {avg_val_loss:.4f}, val MAPE: {mape:.2f}%')\n",
    "        \n",
    "        # Ранняя остановка если MAPE не улучшается\n",
    "        if epoch > 5 and mape > min(history['val_mape'][:-5]) * 1.1:\n",
    "            print(f\"Ранняя остановка на эпохе {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Финальная оценка на тестовой выборке\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, gdp_norm in test_loader:\n",
    "            images = images.to(device)\n",
    "            gdp_norm = gdp_norm.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            test_predictions.extend(outputs.cpu().numpy())\n",
    "            test_actuals.extend(gdp_norm.cpu().numpy())\n",
    "    \n",
    "    # Обратная нормализация\n",
    "    test_predictions = np.array(test_predictions).reshape(-1, 1)\n",
    "    test_actuals = np.array(test_actuals).reshape(-1, 1)\n",
    "    \n",
    "    test_predictions_orig = gdp_scaler.inverse_transform(test_predictions)\n",
    "    test_actuals_orig = gdp_scaler.inverse_transform(test_actuals)\n",
    "    \n",
    "    # Вычисляем метрики\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_actuals_orig, test_predictions_orig))\n",
    "    test_r2 = r2_score(test_actuals_orig, test_predictions_orig)\n",
    "    \n",
    "    mask = test_actuals_orig.flatten() != 0\n",
    "    if mask.any():\n",
    "        test_mape = np.mean(np.abs(\n",
    "            (test_actuals_orig.flatten()[mask] - test_predictions_orig.flatten()[mask]) / \n",
    "            test_actuals_orig.flatten()[mask]\n",
    "        )) * 100\n",
    "    else:\n",
    "        test_mape = np.inf\n",
    "    \n",
    "    print('\\n' + '='*50)\n",
    "    print('ТЕСТОВЫЕ МЕТРИКИ')\n",
    "    print('='*50)\n",
    "    print(f'Test RMSE: {test_rmse:.2f}')\n",
    "    print(f'Test R2: {test_r2:.4f}')\n",
    "    print(f'Test MAPE: {test_mape:.2f}%')\n",
    "    print('='*50)\n",
    "    \n",
    "    # Сохраняем тестовые метрики в историю\n",
    "    history['test_rmse'] = test_rmse\n",
    "    history['test_r2'] = test_r2\n",
    "    history['test_mape'] = test_mape\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def train_cnn_pipeline(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    feature_cols: List[str]\n",
    ") -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"Полный пайплайн обучения CNN для регрессии GDP.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Обучающая выборка\n",
    "        val_df: Валидационная выборка\n",
    "        test_df: Тестовая выборка\n",
    "        feature_cols: Список колонок с признаками\n",
    "        \n",
    "    Returns:\n",
    "        Кортеж (обученная модель, история обучения)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Создание DataLoader'ов...\")\n",
    "    train_loader, val_loader, test_loader, gdp_scaler = create_data_loaders(\n",
    "        train_df, val_df, test_df, feature_cols\n",
    "    )\n",
    "    \n",
    "    print(\"Обучение CNN модели...\")\n",
    "    model, history = train_cnn_model(\n",
    "        train_loader, val_loader, test_loader, gdp_scaler\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Training and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df_merged.columns if col.startswith('feature_')]\n",
    "cnn_model, history = train_cnn_pipeline(train_df, val_df, test_df, feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['val_mape'], label='Val MAPE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.legend()\n",
    "plt.title('Validation MAPE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
