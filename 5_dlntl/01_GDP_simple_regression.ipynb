{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MGIMO intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NTL dataset: baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Library installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for use of OpenCV with Python API [see here](https://docs.opencv.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install opencv-python\n",
    "!pip install -U ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "import re\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image, ImageFile\n",
    "import gc\n",
    "\n",
    "# Включение поддержки загрузки больших изображений\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Take the dataset from Kaggle - [Country-Wise Nightlight Images Dataset](https://www.kaggle.com/datasets/abhijeetdtu/country-nightlight-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jovyan/__DATA/mgimo_intensive/nlt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -la $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = '/home/jovyan/__DATA/mgimo_intensive/nlt/dataset'\n",
    "TARGET_SIZE = (128, 128)\n",
    "LIMIT_IMGS = 500\n",
    "RANDOM_STATE = 42\n",
    "PROFILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(IMG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp = pd.read_csv(f\"{DATA_PATH}/gdp_melted.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROFILE:\n",
    "    profile = ProfileReport(\n",
    "        df_gdp, \n",
    "        title=\"GDP data profiling report\"\n",
    "    )\n",
    "    profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count files and images\n",
    "all_files = list(data_dir.glob('*'))\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}\n",
    "image_files = [f for f in all_files if f.suffix.lower() in image_extensions]\n",
    "\n",
    "print(f\"Total files: {len(all_files)}\")\n",
    "print(f\"Image files: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Open and display a few images\n",
    "if image_files:\n",
    "    # Display up to 5 images\n",
    "    num_to_show = min(4, len(image_files))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_to_show, figsize=(15, 5))\n",
    "    if num_to_show == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, img_file in enumerate(image_files[:num_to_show]):\n",
    "        # Read image with OpenCV\n",
    "        img_path = str(img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is not None:\n",
    "            # Convert BGR to RGB for matplotlib\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Display image\n",
    "            axes[i].imshow(img_rgb)\n",
    "            axes[i].set_title(f\"{img_file.name}\\n{img.shape}\")\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f\"Could not load\\n{img_file.name}\", \n",
    "                       ha='center', va='center')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print image details\n",
    "    print(\"\\nFirst few images details:\")\n",
    "    for img_file in image_files[:num_to_show]:\n",
    "        img = cv2.imread(str(img_file))\n",
    "        if img is not None:\n",
    "            print(f\"  {img_file.name}: shape={img.shape}, size={img_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Simple regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will explore how GDP can be predicted by NTL images.\n",
    "\n",
    "Use prompt to generate code for modelling:\n",
    "\n",
    "```prompt\n",
    "## Базовые параметры\n",
    "- Роль: Junior Python Developer (Data Analyst, CV)\n",
    "- Специализация: data analysis, computer vision\n",
    "- Уровень: начинающий\n",
    "- Температура: 0 (максимальная точность и предсказуемость)\n",
    "\n",
    "## Контекст выполнения\n",
    "Разработка кода для построения модели прогноза на основе изображений\n",
    "\n",
    "## Входные данные\n",
    "- Pandas датафрейм `df_gdp` c данными \n",
    "- Изображения в формате JPEG\n",
    "\n",
    "## Технические ограничения\n",
    "- Использовать стандартные библиотеки Python\n",
    "- Код запускается в интерактивном ноутбуке Jupyter\n",
    "- Необходимо учесть производительность, количество изображений около 3 тысяч, размер изображений примерно 3000 на 7000 пикселей\n",
    "\n",
    "## Требования к реализации\n",
    "\n",
    "### Функциональные требования\n",
    "1. Требуется создание датафрейма `df_img` на основе изображений c с полями, позволяющими соединить этот датафрейм с `df_gdp` датафрейме\n",
    "2. Датафрейм `df_img` должен содержать поля, позволяющие соединить его с `df_gdp`, поле с путями к файлам, данные полученные из названия файла изображения (год, код страны) \n",
    "3. Изображения необходимо уменьшить в разрешении, чтобы ускорить обработку\n",
    "4. Вектор, полученный из каждого изображения будет формировать список признаков, на основе которого будет предсказываться показатель GDP из датафрейма `df_gdp` \n",
    "5. При разделении на обучающую и тестовую выборку используй разделение по странам и временным периодам, для минимизации data leaks\n",
    "6. Для демонстрации использую простую линейную регрессию \n",
    "\n",
    "### Технические требования\n",
    "- Сложность кода: не используй классы, ограничься функциями\n",
    "- Архитектура: упрощенная, для использования в интерактивных ноутбуках\n",
    "- Стиль кода: PEP 8, black (длина строки 79)\n",
    "- Документация: Docstrings в стиле Google\n",
    "- Безопасность: Никаких захардкоженных credentials\n",
    "\n",
    "### Структура данных `df_gdp`\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Index: 16104 entries, 0 to 16103\n",
    "Data columns (total 6 columns):\n",
    " #   Column          Non-Null Count  Dtype  \n",
    "---  ------          --------------  -----  \n",
    " 0   country         16104 non-null  object \n",
    " 1   code            16104 non-null  object \n",
    " 2   indicator       16104 non-null  object \n",
    " 3   indicator_code  16104 non-null  object \n",
    " 4   year            16104 non-null  int64  \n",
    " 5   gdp             12372 non-null  float64\n",
    "dtypes: float64(1), int64(1), object(4)\n",
    "memory usage: 880.7+ KB\n",
    "\n",
    "### Пример данных `df_gdp`\n",
    "  country code  indicator indicator_code  year  gdp\n",
    "0 Aruba ABW GDP (current US$) NY.GDP.MKTP.CD  1960  NaN\n",
    "1 Afghanistan AFG GDP (current US$) NY.GDP.MKTP.CD  1960  537777811.1\n",
    "2 Angola  AGO GDP (current US$) NY.GDP.MKTP.CD  1960  NaN\n",
    "3 Albania ALB GDP (current US$) NY.GDP.MKTP.CD  1960  NaN\n",
    "4 Andorra AND GDP (current US$) NY.GDP.MKTP.CD  1960  NaN\n",
    "\n",
    "### Хранение изображений\n",
    "IMG_PATH = '/home/jovyan/__DATA/mgimo_intensive/nlt/dataset'\n",
    "\n",
    "### Пример наименования файлов изображений\n",
    "  CHN_1997.png_0_4784.jpeg: shape=(4382, 7905, 3), size=768.3 KB\n",
    "  JPN_2005.png_0_1141.jpeg: shape=(2393, 3296, 3), size=152.8 KB\n",
    "  AUS_2008.png_0_7320.jpeg: shape=(4153, 5543, 3), size=492.8 KB\n",
    "  CHN_2001.png_0_3134.jpeg: shape=(4382, 7905, 3), size=783.3 KB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_dataframe(img_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Создает датафрейм с информацией об изображениях.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Путь к директории с изображениями\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame с колонками:\n",
    "        - country_code: код страны из названия файла\n",
    "        - year: год из названия файла\n",
    "        - file_path: полный путь к файлу изображения\n",
    "        - filename: исходное имя файла\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: Если указанная директория не существует\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Директория {img_path} не найдена\")\n",
    "    \n",
    "    # Собираем информацию о файлах\n",
    "    image_data = []\n",
    "    \n",
    "    # Паттерн для извлечения кода страны и года из имени файла\n",
    "    # Пример: CHN_1997.png_0_4784.jpeg\n",
    "    pattern = re.compile(r'([A-Z]{3})_(\\d{4})')\n",
    "    \n",
    "    for filename in os.listdir(img_path):\n",
    "        if not filename.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
    "            continue\n",
    "            \n",
    "        match = pattern.search(filename)\n",
    "        if match:\n",
    "            country_code = match.group(1)\n",
    "            year = int(match.group(2))\n",
    "            \n",
    "            image_data.append({\n",
    "                'country_code': country_code,\n",
    "                'year': year,\n",
    "                'file_path': str(Path(img_path) / filename),\n",
    "                'filename': filename\n",
    "            })\n",
    "    \n",
    "    df_img = pd.DataFrame(image_data)\n",
    "    \n",
    "    if df_img.empty:\n",
    "        print(\"Предупреждение: не найдено изображений, соответствующих паттерну\")\n",
    "    \n",
    "    return df_img\n",
    "\n",
    "\n",
    "def load_and_resize_image(file_path: str, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:\n",
    "    \"\"\"Загружает изображение и изменяет его до точного размера target_size.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Путь к файлу изображения\n",
    "        target_size: Целевой размер (ширина, высота), по умолчанию (224, 224)\n",
    "        \n",
    "    Returns:\n",
    "        Изображение в виде numpy массива (224, 224) с типом float16\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Загружаем изображение сразу в grayscale\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_REDUCED_GRAYSCALE_2)\n",
    "        \n",
    "        if img is None:\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Принудительный ресайз до точного размера 224x224\n",
    "        # INTER_LINEAR хорош для увеличения, INTER_AREA для уменьшения\n",
    "        # Определяем, нужно увеличивать или уменьшать\n",
    "        height, width = img.shape[:2]\n",
    "        if height > target_size[1] or width > target_size[0]:\n",
    "            # Уменьшаем\n",
    "            interpolation = cv2.INTER_AREA\n",
    "        else:\n",
    "            # Увеличиваем\n",
    "            interpolation = cv2.INTER_LINEAR\n",
    "        \n",
    "        # Ресайз до точного размера\n",
    "        img_resized = cv2.resize(img, target_size, interpolation=interpolation)\n",
    "        \n",
    "        # Конвертируем в float16 и нормализуем\n",
    "        img_array = img_resized / 255.0\n",
    "        \n",
    "        # Очищаем память\n",
    "        del img, img_resized\n",
    "        gc.collect()\n",
    "        \n",
    "        return img_array\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_image_features(\n",
    "    image_array: np.ndarray,\n",
    "    method: str = 'flatten'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Извлекает признаки из черно-белого изображения.\n",
    "    \n",
    "    Args:\n",
    "        image_array: Изображение в виде numpy массива (height, width)\n",
    "        method: Метод извлечения признаков\n",
    "        \n",
    "    Returns:\n",
    "        Вектор признаков\n",
    "    \"\"\"\n",
    "    if image_array is None:\n",
    "        return None\n",
    "    \n",
    "    if method == 'flatten':\n",
    "        # Для grayscale просто выпрямляем 2D массив\n",
    "        features = image_array.flatten()\n",
    "    elif method == 'mean_pooling':\n",
    "        # Усреднение по блокам для уменьшения размерности\n",
    "        h, w = image_array.shape\n",
    "        block_size = 8\n",
    "        h_blocks = h // block_size\n",
    "        w_blocks = w // block_size\n",
    "        features = image_array[:h_blocks*block_size, :w_blocks*block_size] \\\n",
    "                          .reshape(h_blocks, block_size, w_blocks, block_size) \\\n",
    "                          .mean(axis=(1, 3)) \\\n",
    "                          .flatten()\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный метод: {method}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def prepare_features_for_country_year(\n",
    "    df_img: pd.DataFrame,\n",
    "    method: str = 'flatten',\n",
    "    limit: int = 200,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Подготавливает признаки для всех изображений.\n",
    "    \n",
    "    Args:\n",
    "        df_img: Датафрейм с информацией об изображениях\n",
    "        method: Метод извлечения признаков\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame с признаками и мета-информацией\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_list = []\n",
    "\n",
    "    if limit == None: limit = len(df_img)\n",
    "    \n",
    "    for idx, row in df_img[:limit].iterrows():\n",
    "        if idx % 100 == 0:  # Прогресс каждые 100 изображений\n",
    "            print(f\"Обработано {idx} из {len(df_img)} изображений\")\n",
    "        \n",
    "        # Загружаем и уменьшаем изображение\n",
    "        img_array = load_and_resize_image(row['file_path'], TARGET_SIZE)\n",
    "        \n",
    "        if img_array is not None:\n",
    "            # Извлекаем признаки\n",
    "            features = extract_image_features(img_array, method)\n",
    "            \n",
    "            # Сохраняем признаки вместе с мета-информацией\n",
    "            feature_dict = {\n",
    "                'country_code': row['country_code'],\n",
    "                'year': row['year'],\n",
    "                'file_path': row['file_path']\n",
    "            }\n",
    "            \n",
    "            # Добавляем признаки как отдельные колонки\n",
    "            for i, feature_value in enumerate(features):\n",
    "                feature_dict[f'feature_{i}'] = feature_value\n",
    "            \n",
    "            feature_list.append(feature_dict)\n",
    "    \n",
    "    df_features = pd.DataFrame(feature_list)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def merge_with_gdp_data(\n",
    "    df_features: pd.DataFrame,\n",
    "    df_gdp: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Объединяет признаки с данными о ВВП.\n",
    "    \n",
    "    Args:\n",
    "        df_features: DataFrame с признаками изображений\n",
    "        df_gdp: DataFrame с данными о ВВП\n",
    "        \n",
    "    Returns:\n",
    "        Объединенный DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Подготавливаем данные ВВП\n",
    "    df_gdp_filtered = df_gdp[['code', 'year', 'gdp']].copy()\n",
    "    df_gdp_filtered = df_gdp_filtered.dropna(subset=['gdp'])\n",
    "    \n",
    "    # Переименовываем для объединения\n",
    "    df_gdp_filtered = df_gdp_filtered.rename(columns={'code': 'country_code'})\n",
    "    \n",
    "    # Объединяем датафреймы\n",
    "    df_merged = pd.merge(\n",
    "        df_features,\n",
    "        df_gdp_filtered,\n",
    "        on=['country_code', 'year'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"Объединено записей: {len(df_merged)}\")\n",
    "    print(f\"Уникальных стран: {df_merged['country_code'].nunique()}\")\n",
    "    print(f\"Диапазон лет: {df_merged['year'].min()} - {df_merged['year'].max()}\")\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def split_data_by_country_and_year(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Разделяет данные на train/val/test с учетом стран и временных периодов.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame с данными\n",
    "        test_size: Доля тестовой выборки\n",
    "        val_size: Доля валидационной выборки\n",
    "        \n",
    "    Returns:\n",
    "        Кортеж (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем группы по странам\n",
    "    groups = df['country_code'].values\n",
    "    \n",
    "    # Первое разделение: выделяем тестовую выборку\n",
    "    gss1 = GroupShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=test_size,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    train_val_idx, test_idx = next(\n",
    "        gss1.split(df, groups=groups)\n",
    "    )\n",
    "    \n",
    "    train_val_df = df.iloc[train_val_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    \n",
    "    # Второе разделение: выделяем валидационную из train\n",
    "    val_relative_size = val_size / (1 - test_size)\n",
    "    gss2 = GroupShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=val_relative_size,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    train_idx, val_idx = next(\n",
    "        gss2.split(\n",
    "            train_val_df,\n",
    "            groups=train_val_df['country_code'].values\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_df = train_val_df.iloc[train_idx]\n",
    "    val_df = train_val_df.iloc[val_idx]\n",
    "    \n",
    "    print(f\"Train: {len(train_df)} samples, {train_df['country_code'].nunique()} countries\")\n",
    "    print(f\"Val: {len(val_df)} samples, {val_df['country_code'].nunique()} countries\")\n",
    "    print(f\"Test: {len(test_df)} samples, {test_df['country_code'].nunique()} countries\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def prepare_X_y(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    target_col: str = 'gdp'\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Подготавливает матрицу признаков и целевую переменную.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame с данными\n",
    "        feature_cols: Список колонок с признаками\n",
    "        target_col: Название колонки с целевой переменной\n",
    "        \n",
    "    Returns:\n",
    "        Кортеж (X, y)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Вычисляет Mean Absolute Percentage Error (MAPE).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Фактические значения\n",
    "        y_pred: Предсказанные значения\n",
    "        \n",
    "    Returns:\n",
    "        MAPE в процентах\n",
    "    \"\"\"\n",
    "    # Избегаем деления на ноль\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Фильтруем нулевые значения\n",
    "    mask = y_true != 0\n",
    "    if not mask.any():\n",
    "        return np.inf\n",
    "    \n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    return mape\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    model_type: str = 'linear'\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Обучает и оценивает модель (линейная регрессия или Random Forest).\n",
    "    \n",
    "    Args:\n",
    "        train_df: Обучающая выборка\n",
    "        val_df: Валидационная выборка\n",
    "        test_df: Тестовая выборка\n",
    "        feature_cols: Список колонок с признаками\n",
    "        model_type: Тип модели ('linear' или 'random_forest')\n",
    "        \n",
    "    Returns:\n",
    "        Словарь с метриками\n",
    "    \"\"\"\n",
    "    \n",
    "    # Подготавливаем данные\n",
    "    X_train, y_train = prepare_X_y(train_df, feature_cols)\n",
    "    X_val, y_val = prepare_X_y(val_df, feature_cols)\n",
    "    X_test, y_test = prepare_X_y(test_df, feature_cols)\n",
    "    \n",
    "    # Масштабируем признаки (только для линейной регрессии)\n",
    "    if model_type == 'linear':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        # Для Random Forest масштабирование не требуется\n",
    "        X_train_scaled = X_train\n",
    "        X_val_scaled = X_val\n",
    "        X_test_scaled = X_test\n",
    "        scaler = None\n",
    "    \n",
    "    # Выбираем и обучаем модель\n",
    "    if model_type == 'linear':\n",
    "        model = LinearRegression()\n",
    "    elif model_type == 'random_forest':\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,      # Количество деревьев\n",
    "            max_depth=10,           # Максимальная глубина\n",
    "            min_samples_split=5,    # Минимальное количество样本 для разделения\n",
    "            min_samples_leaf=2,     # Минимальное количество样本 в листе\n",
    "            n_jobs=-1,              # Использовать все ядра процессора\n",
    "            random_state=42,\n",
    "            verbose=0               # Отключаем вывод прогресса\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный тип модели: {model_type}\")\n",
    "    \n",
    "    # Обучаем модель\n",
    "    print(f\"Обучение модели {model_type}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Делаем предсказания\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Вычисляем метрики\n",
    "    metrics = {\n",
    "        # R2\n",
    "        'train_r2': r2_score(y_train, y_train_pred),\n",
    "        'val_r2': r2_score(y_val, y_val_pred),\n",
    "        'test_r2': r2_score(y_test, y_test_pred),\n",
    "        \n",
    "        # RMSE\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        \n",
    "        # MAPE\n",
    "        'train_mape': mean_absolute_percentage_error(y_train, y_train_pred),\n",
    "        'val_mape': mean_absolute_percentage_error(y_val, y_val_pred),\n",
    "        'test_mape': mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    # Для Random Forest добавляем важность признаков\n",
    "    if model_type == 'random_forest':\n",
    "        # Получаем важность признаков\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nТоп-10 наиболее важных признаков:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        # Добавляем информацию о важности признаков в метрики\n",
    "        metrics['top_features'] = feature_importance.head(10).to_dict()\n",
    "    \n",
    "    return metrics, model, scaler\n",
    "\n",
    "\n",
    "def plot_predictions(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    title: str = \"Предсказанные vs Фактические значения\"\n",
    ") -> None:\n",
    "    \"\"\"Строит график предсказанных vs фактических значений.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Фактические значения\n",
    "        y_pred: Предсказанные значения\n",
    "        title: Заголовок графика\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    \n",
    "    # Линия идеального предсказания\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    \n",
    "    plt.xlabel('Фактические значения')\n",
    "    plt.ylabel('Предсказанные значения')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ШАГ 1: Создание датафрейма изображений\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_img = create_image_dataframe(IMG_PATH)\n",
    "print(f\"Найдено изображений: {len(df_img)}\")\n",
    "print(f\"Уникальных стран: {df_img['country_code'].nunique()}\")\n",
    "print(f\"Диапазон лет: {df_img['year'].min()} - {df_img['year'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 2: Извлечение признаков из изображений\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_features = prepare_features_for_country_year(df_img, method='flatten', limit=LIMIT_IMGS)\n",
    "print(f\"Размерность признаков: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 3: Объединение с данными ВВП\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_merged = merge_with_gdp_data(df_features, df_gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 4: Разделение на выборки\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_df, val_df, test_df = split_data_by_country_and_year(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 5: Обучение модели\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Определяем колонки с признаками\n",
    "feature_cols = [col for col in df_merged.columns \n",
    "                if col.startswith('feature_')]\n",
    "\n",
    "metrics, model, scaler = train_and_evaluate_model(\n",
    "    train_df, val_df, test_df, feature_cols, model_type='linear'\n",
    ")\n",
    "\n",
    "print(\"\\nМетрики модели:\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 6: Визуализация результатов\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Предсказания на тестовой выборке\n",
    "X_test, y_test = prepare_X_y(test_df, feature_cols)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "plot_predictions(y_test, y_test_pred, \"Тестовая выборка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 7: Обучение модели Random Forest\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Определяем колонки с признаками\n",
    "feature_cols = [col for col in df_merged.columns \n",
    "                if col.startswith('feature_')]\n",
    "\n",
    "metrics, model, scaler = train_and_evaluate_model(\n",
    "    train_df, val_df, test_df, feature_cols, model_type='random_forest'\n",
    ")\n",
    "\n",
    "print(\"\\nМетрики модели:\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    try:\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "    except: pass\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ШАГ 8: Визуализация результатов\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Предсказания на тестовой выборке\n",
    "X_test, y_test = prepare_X_y(test_df, feature_cols)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "plot_predictions(y_test, y_test_pred, \"Тестовая выборка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
