{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGIMO intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Apache Spark](imgs/spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web UI for the Spark\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# Spark settings\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.driver.memory', '4G')\n",
    "conf.set('spark.driver.maxResultSize', '2G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data_section37_112_v20250918_section_file/data_section37_112_v20250918.parquet\"\n",
    "sdf = spark.read.parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"./data_section35_112_v20250918_section_file/data_section35_112_v20250918.parquet\",\n",
    "    \"./data_section37_112_v20250918_section_file/data_section37_112_v20250918.parquet\",\n",
    "    \"./data_section39_112_v20250918_section_file/data_section39_112_v20250918.parquet\"\n",
    "]\n",
    "sdf = spark.read.parquet(*file_paths)\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analytics with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt will help us:\n",
    "\n",
    "```prompt\n",
    "## Базовые параметры\n",
    "- Роль: Junior Python Developer (Data Analyst)\n",
    "- Специализация: data analysis, data visualization, data collection\n",
    "- Уровень: начинающий\n",
    "- Температура: 0 (максимальная точность и предсказуемость)\n",
    "\n",
    "## Контекст выполнения\n",
    "Разработка кода для анализа больших данных\n",
    "\n",
    "## Входные данные\n",
    "- PySpark датафрейм `sdf` c данными \n",
    "- Формат файла: `parquet`\n",
    "\n",
    "## Технические ограничения\n",
    "- Использовать PySpark и стандартные библиотеки Python\n",
    "- Код запускается в интерактивном ноутбуке Jupyter\n",
    "- Необходимо учесть производительность, количество записей 5 млн\n",
    "\n",
    "## Требования к реализации\n",
    "\n",
    "### Функциональные требования\n",
    "1. Требуется аналитика по основным показателям, доступным в датафрейме\n",
    "2. Требуется общая описательная аналитика \n",
    "3. Визуализация стандартными библиотеками \n",
    "4. Необходимо разбить анализ на последовательные блоки (общие показатели, отдельные тренды, визуализация, гипотезы)\n",
    "\n",
    "### Технические требования\n",
    "- Сложность кода: не используй классы, ограничься функциями\n",
    "- Архитектура: упрощенная, для использования в интерактивных ноутбуках\n",
    "- Стиль кода: PEP 8, black (длина строки 79)\n",
    "- Документация: Docstrings в стиле Google\n",
    "- Безопасность: Никаких захардкоженных credentials\n",
    "\n",
    "### Структура данных \n",
    "sdf.printSchema()\n",
    "<ВЫВОД КОМАНДЫ>\n",
    "\n",
    "### Требования к библиотекам\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# БЛОК 1: ОБЩИЕ ПОКАЗАТЕЛИ ДАТАФРЕЙМА\n",
    "# =============================================================================\n",
    "\n",
    "def get_basic_statistics(df):\n",
    "    \"\"\"\n",
    "    Расчет базовых статистик датафрейма.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary с основными метриками\n",
    "    \"\"\"\n",
    "    basic_stats = {}\n",
    "    \n",
    "    # Количество уникальных значений в ключевых колонках\n",
    "    basic_stats['total_records'] = df.count()\n",
    "    basic_stats['unique_regions'] = df.select('region_name').distinct().count()\n",
    "    basic_stats['unique_municipalities'] = df.select('municipality').distinct().count()\n",
    "    basic_stats['unique_indicators'] = df.select('indicator_code').distinct().count()\n",
    "    basic_stats['year_range'] = df.agg(\n",
    "        F.min('year').alias('min_year'),\n",
    "        F.max('year').alias('max_year')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Проверка на пропущенные значения\n",
    "    null_counts = df.select([\n",
    "        F.sum(F.col(c).isNull().cast('int')).alias(c) \n",
    "        for c in ['indicator_value', 'region_name', 'municipality', 'year']\n",
    "    ]).collect()[0]\n",
    "    \n",
    "    basic_stats['null_values'] = null_counts.asDict()\n",
    "    \n",
    "    return basic_stats\n",
    "\n",
    "# Вывод общей статистики\n",
    "basic_stats = get_basic_statistics(sdf)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ОБЩАЯ СТАТИСТИКА ДАННЫХ\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Всего записей: {basic_stats['total_records']:,}\")\n",
    "print(f\"Количество регионов: {basic_stats['unique_regions']}\")\n",
    "print(f\"Количество муниципалитетов: {basic_stats['unique_municipalities']}\")\n",
    "print(f\"Количество показателей: {basic_stats['unique_indicators']}\")\n",
    "print(f\"Период данных: {basic_stats['year_range']['min_year']} - {basic_stats['year_range']['max_year']}\")\n",
    "print(f\"Пропущенные значения: {basic_stats['null_values']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# БЛОК 2: ОПИСАТЕЛЬНАЯ СТАТИСТИКА ПОКАЗАТЕЛЕЙ\n",
    "# =============================================================================\n",
    "\n",
    "def get_descriptive_stats(df):\n",
    "    \"\"\"\n",
    "    Расчет описательных статистик для числовых показателей.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        PySpark DataFrame с описательными статистиками\n",
    "    \"\"\"\n",
    "    # Статистики по каждому показателю\n",
    "    descriptive_stats = df.groupBy('indicator_code', 'indicator_name') \\\n",
    "        .agg(\n",
    "            F.count('indicator_value').alias('count'),\n",
    "            F.avg('indicator_value').alias('mean'),\n",
    "            F.stddev('indicator_value').alias('stddev'),\n",
    "            F.min('indicator_value').alias('min'),\n",
    "            F.percentile_approx('indicator_value', 0.25).alias('q1'),\n",
    "            F.percentile_approx('indicator_value', 0.5).alias('median'),\n",
    "            F.percentile_approx('indicator_value', 0.75).alias('q3'),\n",
    "            F.max('indicator_value').alias('max')\n",
    "        ) \\\n",
    "        .orderBy('indicator_code')\n",
    "    \n",
    "    return descriptive_stats\n",
    "\n",
    "# Получение описательной статистики\n",
    "desc_stats = get_descriptive_stats(sdf)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ОПИСАТЕЛЬНАЯ СТАТИСТИКА ПОКАЗАТЕЛЕЙ\")\n",
    "print(\"=\"*50)\n",
    "#desc_stats.show(10, truncate=False)\n",
    "display(desc_stats.limit(10).toPandas())\n",
    "\n",
    "# Анализ по годам\n",
    "def yearly_summary(df):\n",
    "    \"\"\"\n",
    "    Сводка по годам: количество записей и средние значения.\n",
    "    \"\"\"\n",
    "    yearly_stats = df.groupBy('year') \\\n",
    "        .agg(\n",
    "            F.count('*').alias('records_count'),\n",
    "            F.avg('indicator_value').alias('avg_indicator_value'),\n",
    "            F.countDistinct('indicator_code').alias('unique_indicators'),\n",
    "            F.countDistinct('municipality').alias('municipalities_count')\n",
    "        ) \\\n",
    "        .orderBy('year')\n",
    "    \n",
    "    return yearly_stats\n",
    "\n",
    "yearly_stats = yearly_summary(sdf)\n",
    "print(\"\\nСтатистика по годам:\")\n",
    "display(yearly_stats.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# БЛОК 3: ВИЗУАЛИЗАЦИЯ ОСНОВНЫХ ТРЕНДОВ\n",
    "# =============================================================================\n",
    "\n",
    "def plot_yearly_trends(df, top_n=5):\n",
    "    \"\"\"\n",
    "    Визуализация трендов по годам для топ показателей.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "        top_n: количество показателей для визуализации\n",
    "    \"\"\"\n",
    "    # Определяем топ показателей по количеству записей\n",
    "    top_indicators = df.groupBy('indicator_code', 'indicator_name') \\\n",
    "        .agg(F.count('*').alias('count')) \\\n",
    "        .orderBy(F.desc('count')) \\\n",
    "        .limit(top_n) \\\n",
    "        .select('indicator_code') \\\n",
    "        .rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Собираем данные для визуализации\n",
    "    trend_data = []\n",
    "    for indicator in top_indicators:\n",
    "        indicator_data = df.filter(F.col('indicator_code') == indicator) \\\n",
    "            .groupBy('year') \\\n",
    "            .agg(F.avg('indicator_value').alias('avg_value')) \\\n",
    "            .orderBy('year') \\\n",
    "            .toPandas()\n",
    "        \n",
    "        indicator_data['indicator'] = indicator\n",
    "        trend_data.append(indicator_data)\n",
    "    \n",
    "    # Создание визуализации\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # График 1: Тренды топ показателей\n",
    "    ax1 = axes[0]\n",
    "    for data in trend_data:\n",
    "        if not data.empty:\n",
    "            ax1.plot(data['year'], data['avg_value'], marker='o', label=data['indicator'].iloc[0])\n",
    "    \n",
    "    ax1.set_xlabel('Год')\n",
    "    ax1.set_ylabel('Среднее значение')\n",
    "    ax1.set_title('Динамика топ показателей по годам')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # График 2: Распределение записей по годам\n",
    "    ax2 = axes[1]\n",
    "    yearly_counts = df.groupBy('year').count().orderBy('year').toPandas()\n",
    "    ax2.bar(yearly_counts['year'], yearly_counts['count'], color='skyblue', edgecolor='navy')\n",
    "    ax2.set_xlabel('Год')\n",
    "    ax2.set_ylabel('Количество записей')\n",
    "    ax2.set_title('Распределение записей по годам')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_regional_distribution(df):\n",
    "    \"\"\"\n",
    "    Визуализация распределения данных по регионам.\n",
    "    \"\"\"\n",
    "    # Топ регионов по количеству записей\n",
    "    region_stats = df.groupBy('region_name') \\\n",
    "        .agg(F.count('*').alias('records')) \\\n",
    "        .orderBy(F.desc('records')) \\\n",
    "        .limit(15) \\\n",
    "        .toPandas()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Горизонтальная бар-чарт\n",
    "    ax1 = axes[0]\n",
    "    y_pos = np.arange(len(region_stats))\n",
    "    ax1.barh(y_pos, region_stats['records'])\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(region_stats['region_name'])\n",
    "    ax1.set_xlabel('Количество записей')\n",
    "    ax1.set_title('Топ-15 регионов по количеству записей')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Круговая диаграмма распределения по типам муниципалитетов\n",
    "    ax2 = axes[1]\n",
    "    mun_type_stats = df.groupBy('mun_type') \\\n",
    "        .agg(F.count('*').alias('count')) \\\n",
    "        .orderBy(F.desc('count')) \\\n",
    "        .toPandas()\n",
    "    \n",
    "    # Ограничим количество категорий для читаемости\n",
    "    other_sum = mun_type_stats[mun_type_stats['count'] < mun_type_stats['count'].quantile(0.7)]['count'].sum()\n",
    "    main_categories = mun_type_stats[mun_type_stats['count'] >= mun_type_stats['count'].quantile(0.7)]\n",
    "    \n",
    "    if other_sum > 0:\n",
    "        main_categories = pd.concat([\n",
    "            main_categories,\n",
    "            pd.DataFrame({'mun_type': ['Другие'], 'count': [other_sum]})\n",
    "        ])\n",
    "    \n",
    "    ax2.pie(main_categories['count'], labels=main_categories['mun_type'], autopct='%1.1f%%')\n",
    "    ax2.set_title('Распределение по типам муниципалитетов')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Вызов функций визуализации\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ВИЗУАЛИЗАЦИЯ ДАННЫХ\")\n",
    "print(\"=\"*50)\n",
    "plot_yearly_trends(sdf)\n",
    "plot_regional_distribution(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# БЛОК 4: АНАЛИЗ ГИПОТЕЗ\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_correlations(df):\n",
    "    \"\"\"\n",
    "    Анализ корреляций между показателями.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "    \"\"\"\n",
    "    # Выбираем топ показателей по встречаемости\n",
    "    top_indicators = df.groupBy('indicator_code') \\\n",
    "        .agg(F.count('*').alias('count')) \\\n",
    "        .orderBy(F.desc('count')) \\\n",
    "        .limit(10) \\\n",
    "        .select('indicator_code') \\\n",
    "        .rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Создаем сводную таблицу для корреляционного анализа\n",
    "    pivot_df = df.filter(F.col('indicator_code').isin(top_indicators)) \\\n",
    "        .groupBy('municipality', 'year') \\\n",
    "        .pivot('indicator_code') \\\n",
    "        .agg(F.first('indicator_value'))\n",
    "    \n",
    "    # Конвертируем в pandas для корреляционного анализа (только для небольшого объема)\n",
    "    # Внимание: используйте только для агрегированных данных, не для полного датафрейма\n",
    "    pandas_df = pivot_df.limit(1000).toPandas()\n",
    "    \n",
    "    # Удаляем нечисловые колонки для корреляции\n",
    "    numeric_cols = pandas_df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr_matrix = pandas_df[numeric_cols].corr()\n",
    "        \n",
    "        # Визуализация корреляционной матрицы\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        im = ax.imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "        \n",
    "        # Настройка отображения\n",
    "        ax.set_xticks(np.arange(len(numeric_cols)))\n",
    "        ax.set_yticks(np.arange(len(numeric_cols)))\n",
    "        ax.set_xticklabels(numeric_cols, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(numeric_cols)\n",
    "        \n",
    "        plt.colorbar(im)\n",
    "        plt.title('Корреляционная матрица топ показателей')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return corr_matrix\n",
    "    \n",
    "    return None\n",
    "\n",
    "def check_data_quality_hypothesis(df):\n",
    "    \"\"\"\n",
    "    Проверка гипотез о качестве данных.\n",
    "    \"\"\"\n",
    "    hypotheses_results = {}\n",
    "    \n",
    "    # Гипотеза 1: Пропуски в данных увеличиваются со временем\n",
    "    nulls_by_year = df.groupBy('year') \\\n",
    "        .agg(\n",
    "            F.sum(F.col('indicator_value').isNull().cast('int')).alias('nulls_count'),\n",
    "            F.count('*').alias('total_count')\n",
    "        ) \\\n",
    "        .withColumn('nulls_percentage', F.col('nulls_count') / F.col('total_count') * 100) \\\n",
    "        .orderBy('year') \\\n",
    "        .toPandas()\n",
    "    \n",
    "    hypotheses_results['nulls_trend'] = nulls_by_year\n",
    "    \n",
    "    # Визуализация\n",
    "    if not nulls_by_year.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(nulls_by_year['year'], nulls_by_year['nulls_percentage'], \n",
    "                marker='o', linewidth=2, markersize=8)\n",
    "        ax.set_xlabel('Год')\n",
    "        ax.set_ylabel('Процент пропусков (%)')\n",
    "        ax.set_title('Динамика пропусков в данных по годам')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # Гипотеза 2: Единицы измерения согласованы для каждого показателя\n",
    "    units_check = df.groupBy('indicator_code') \\\n",
    "        .agg(F.countDistinct('indicator_unit').alias('unique_units')) \\\n",
    "        .filter(F.col('unique_units') > 1) \\\n",
    "        .count()\n",
    "    \n",
    "    hypotheses_results['inconsistent_units'] = units_check > 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"РЕЗУЛЬТАТЫ ПРОВЕРКИ ГИПОТЕЗ\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Наличие несогласованных единиц измерения: {hypotheses_results['inconsistent_units']}\")\n",
    "    \n",
    "    return hypotheses_results\n",
    "\n",
    "# Выполнение анализа гипотез\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"АНАЛИЗ ГИПОТЕЗ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Корреляционный анализ (осторожно с большими данными)\n",
    "try:\n",
    "    corr_results = analyze_correlations(sdf)\n",
    "    if corr_results is not None:\n",
    "        print(\"Корреляционный анализ выполнен успешно\")\n",
    "except Exception as e:\n",
    "    print(f\"Корреляционный анализ пропущен: {e}\")\n",
    "\n",
    "# Проверка гипотез о качестве данных\n",
    "quality_results = check_data_quality_hypothesis(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ИТОГОВЫЙ ОТЧЕТ\n",
    "# =============================================================================\n",
    "\n",
    "def generate_summary_report(df, basic_stats, desc_stats):\n",
    "    \"\"\"\n",
    "    Генерация итогового отчета.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ИТОГОВЫЙ АНАЛИТИЧЕСКИЙ ОТЧЕТ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n1. ОБЩАЯ ИНФОРМАЦИЯ:\")\n",
    "    print(f\"   - Объем данных: {basic_stats['total_records']:,} записей\")\n",
    "    print(f\"   - Период: {basic_stats['year_range']['min_year']}-{basic_stats['year_range']['max_year']}\")\n",
    "    print(f\"   - Регионов: {basic_stats['unique_regions']}\")\n",
    "    print(f\"   - Муниципалитетов: {basic_stats['unique_municipalities']}\")\n",
    "    print(f\"   - Показателей: {basic_stats['unique_indicators']}\")\n",
    "    \n",
    "    print(\"\\n2. КАЧЕСТВО ДАННЫХ:\")\n",
    "    print(f\"   - Пропуски в ключевых полях: {basic_stats['null_values']}\")\n",
    "    \n",
    "    print(\"\\n3. КЛЮЧЕВЫЕ ВЫВОДЫ:\")\n",
    "    \n",
    "    # Анализ динамики\n",
    "    yearly_growth = df.groupBy('year') \\\n",
    "        .agg(F.avg('indicator_value').alias('avg_value')) \\\n",
    "        .orderBy('year') \\\n",
    "        .collect()\n",
    "    \n",
    "    if len(yearly_growth) > 1:\n",
    "        first_year = yearly_growth[0]['avg_value']\n",
    "        last_year = yearly_growth[-1]['avg_value']\n",
    "        growth = ((last_year - first_year) / first_year * 100) if first_year else 0\n",
    "        print(f\"   - Общий рост показателей за период: {growth:.1f}%\")\n",
    "    \n",
    "    print(\"\\n4. РЕКОМЕНДАЦИИ:\")\n",
    "    print(\"   - Проверить согласованность единиц измерения для всех показателей\")\n",
    "    print(\"   - Обратить внимание на регионы с наибольшим количеством пропусков\")\n",
    "    print(\"   - Рассмотреть возможность агрегации данных для ускорения анализа\")\n",
    "    \n",
    "    # Освобождение кеша\n",
    "    df.unpersist()\n",
    "\n",
    "# Генерация итогового отчета\n",
    "generate_summary_report(sdf, basic_stats, desc_stats)\n",
    "\n",
    "print(\"\\nАнализ завершен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
